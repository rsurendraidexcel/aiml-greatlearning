{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4808b7",
   "metadata": {},
   "source": [
    "## Spark-DataFrame  Fundamental Practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2e6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f071f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the PySpark environment veriable\n",
    "os.environ['JAVA_HOME']=\"E:\\jdk-22\"\n",
    "os.environ['SPARK_HOME'] = \"E:\\spark-3.5.1-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"jupyter\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPS'] = \"notebook\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67057373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sart the Sparksession\n",
    "spark = SparkSession.builder.appName(\"Practice\")\\\n",
    ".config(\"spark.executor.memory\",\"4g\")\\\n",
    ".config(\"spark.sql.shuffle.partitions\",\"4\")\\\n",
    ".config(\"spark.jars\",\"E:\\jdbc-driver\\postgresql-42.7.3.jar\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04952b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ce2048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BE50MUO:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Practice>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9141a1",
   "metadata": {},
   "source": [
    "### 1. Creating DataFrame and Loading CSV, Text, Json,  Parquet File Into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "data = [\n",
    "  ('James','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','2000-05-19','M',4000),\n",
    "  ('Robert','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Brown','1980-02-17','F',8500),\n",
    "  ('Keith','Lina','1982-09-26','F',8000)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "pyspark_df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the list of Schema\n",
    "pyspark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9c0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show data values\n",
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b20e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read git from csv and store pysparkDataFrame\n",
    "url_github = r\"https://raw.githubusercontent.com/muttinenisairohith/Datasets/b0bb96f293adbb803e24c26b7780e078372d3703/data/test2.csv\"\n",
    "pd_df = pd.read_csv(url_github)\n",
    "df_pyspark1 = spark.createDataFrame(pd_df)\n",
    "df_pyspark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e384e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by Departments which gives summation of salaries\n",
    "df_pyspark1.groupBy(\"Departments\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ea747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark1.groupBy(\"Departments\").agg(({\"salary\":\"sum\"})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark1.groupBy(\"Departments\").min(\"salary\").show()\n",
    "df_pyspark1.groupBy(\"Departments\").max(\"salary\").show()\n",
    "df_pyspark1.groupBy(\"Departments\").avg(\"salary\").show()\n",
    "df_pyspark1.groupBy(\"Departments\").mean(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SparkFiles Load the dataframe\n",
    "from pyspark import SparkFiles\n",
    "student_info = r\"https://raw.githubusercontent.com/AISCIENCES/course-master-big-data-with-pyspark-and-aws/main/Code/03-Spark%20DFs/StudentData.csv\"\n",
    "spark.sparkContext.addFile(student_info)\n",
    "student_df = spark.read.csv(SparkFiles.get(\"StudentData.csv\"),inferSchema=True, header=True)\n",
    "student_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77168b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(student_df.count())\n",
    "print(len(student_df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb81ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows based on null values\n",
    "student_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Pandase Data Frame from sparkDataframe\n",
    "student_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0ba73",
   "metadata": {},
   "source": [
    "## Using RDDs texFile Reading storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data = spark.sparkContext.textFile(\"./data/data.txt\")\n",
    "result_rdd = rdd_data.flatMap(lambda line: line.split(\" \"))\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f81687",
   "metadata": {},
   "source": [
    "### Using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b43d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df_textData = spark.read.text(\"./data/data.txt\")\n",
    "result_textData = df_textData.selectExpr(\"explode(split(value, ' ')) as word\") \\\n",
    "    .groupBy(\"word\").count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704eefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_textData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_textData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f0691",
   "metadata": {},
   "source": [
    "## Read CSV file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "head -10 ./data/products.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d906b",
   "metadata": {},
   "source": [
    "## Read CSV with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83296d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "product_df = spark.read.csv(csv_file_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9909fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "product_df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "product_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104410d",
   "metadata": {},
   "source": [
    "### Read CSV with an explicit schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c017e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"category\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"quantity\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"price\", dataType=DoubleType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Read CSV file into DataFrame with schema definition\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "product_df1 = spark.read.csv(csv_file_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "product_df1.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "product_df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846ef8f",
   "metadata": {},
   "source": [
    "#### Read CSV with using inferSchema Atomatically define schema type by using InferSchema=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with inferSchema\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "product_df2 = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c61596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "product_df2.printSchema()\n",
    "# Display content of DataFrame\n",
    "product_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3beec",
   "metadata": {},
   "source": [
    "### Read JSON file into DataFrame\n",
    "### Single Line JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -15 ./data/products_singleline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22747d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read single line JSON\n",
    "# Each row is a JSON record, records are separated by new line\n",
    "json_file_path = \"./data/products_singleline.json\"\n",
    "json_data_df = spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90193df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "json_data_df.printSchema()\n",
    "# Display content of DataFrame\n",
    "json_data_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f0c1b",
   "metadata": {},
   "source": [
    "## Multi-lines JSON and hass of Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252fb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -20 ./data/products_multiline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multi-line JSON\n",
    "# JSON is an array of record, records are separated by a comma.\n",
    "# each record is defined in multiple lines\n",
    "json_file_path = \"./data/products_multiline.json\"\n",
    "json_data_df = spark.read.json(json_file_path, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9527bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "json_data_df.printSchema()\n",
    "# Display content of DataFrame\n",
    "json_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37242d",
   "metadata": {},
   "source": [
    "## Save in Parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e623d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe into parquet file First time execute second time will fail\n",
    "parquet_file_path = \"./data/products.parquet\"\n",
    "#json_data_df.write.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad5ed0",
   "metadata": {},
   "source": [
    "## Read parquet file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53709b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Parquet fiels \n",
    "parquet_df = spark.read.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09886de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "parquet_df.printSchema()\n",
    "# Display content of DataFrame\n",
    "parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03341c4d",
   "metadata": {},
   "source": [
    "## 2. DataFrame Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e543a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -10 ./data/stocks.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6023f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/stocks.txt\"\n",
    "stocks_data_df = spark.read.csv(data_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53460290",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data_df.printSchema()\n",
    "stocks_data_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88eff13",
   "metadata": {},
   "source": [
    "### Select: Choose specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns \n",
    "selected_columns = stocks_data_df.select(\"id\", \"name\", \"price\")\n",
    "print(\"Selected Columns:\")\n",
    "selected_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79e732",
   "metadata": {},
   "source": [
    "### Filter: Apply conditions to filter rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2199b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered_data = stocks_data_df.filter(stocks_data_df.quantity > 15)\n",
    "print(\"Filtered Data:\", filtered_data.count())\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc167d6",
   "metadata": {},
   "source": [
    "**GroupBy: Group data based on specific columns**\n",
    "\n",
    "**Aggregations: Perform functions like sum, average, etc., on grouped data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy and Aggregations\n",
    "grouped_data = stocks_data_df.groupBy(\"category\").agg({\"quantity\": \"sum\", \"price\": \"avg\"})\n",
    "print(\"Grouped and Aggregated Data:\")\n",
    "grouped_data.show()\n",
    "\n",
    "print(\"Count Row no:\",grouped_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f999ea",
   "metadata": {},
   "source": [
    "### Join: Combine multiple DataFrames based on specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with another DataFrame\n",
    "df2 = stocks_data_df.select(\"id\", \"category\").limit(15)\n",
    "joined_data = stocks_data_df.join(df2, \"id\", \"inner\")\n",
    "print(\"Joined Data:\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ba6ef",
   "metadata": {},
   "source": [
    "### Sort: Arrange rows based on one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683379a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a column\n",
    "sorted_data = stocks_data_df.orderBy(\"price\")\n",
    "print(\"Sorted Data:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a column desc\n",
    "from pyspark.sql.functions import col, desc\n",
    "sorted_data = stocks_data_df.orderBy(col(\"price\").desc(), col(\"id\").desc())\n",
    "print(\"Sorted Data Descending:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17bc0f",
   "metadata": {},
   "source": [
    "## Distinct: Get unique rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6655310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distinct product category\n",
    "distinct_rows = stocks_data_df.select(\"category\").distinct()\n",
    "print(\"Distinct Product Categories:\")\n",
    "distinct_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd591c",
   "metadata": {},
   "source": [
    "### Drop: Remove specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9825f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "dropped_columns = stocks_data_df.drop(\"quantity\", \"category\")\n",
    "print(\"Dropped Columns:\")\n",
    "dropped_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54853d1a",
   "metadata": {},
   "source": [
    "## WithColumn: Add new calculated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new calculated column\n",
    "df_with_new_column = stocks_data_df.withColumn(\"revenue\", stocks_data_df.quantity * stocks_data_df.price)\n",
    "print(\"DataFrame with New Column:\")\n",
    "df_with_new_column.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07384e8b",
   "metadata": {},
   "source": [
    "### Alias: Rename columns for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns using alias\n",
    "df_with_alias = stocks_data_df.withColumnRenamed(\"price\", \"product_price\").withColumnRenamed('category', 'Category_Name')\n",
    "print(\"DataFrame with Aliased Column:\")\n",
    "df_with_alias.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f584d68d",
   "metadata": {},
   "source": [
    "## 3. Spark SQL Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -10 ./data/persons.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeae582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/persons.csv\"\n",
    "person_data_df = spark.read.csv(data_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema of DataFrame\n",
    "person_data_df.printSchema()\n",
    "# Show the initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "person_data_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3770d",
   "metadata": {},
   "source": [
    "## Register the DataFrame as a Temporary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a Temporary Table\n",
    "person_data_df.createOrReplaceTempView(\"person_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338b487",
   "metadata": {},
   "source": [
    "## Now, We Can Perform SQL-like Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52dc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows where age is greater than 25\n",
    "result = spark.sql(\"SELECT * FROM person_table WHERE age > 25\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a839d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average salary by gender\n",
    "avg_salary_by_gender = spark.sql(\"SELECT gender, AVG(salary) as avg_salary FROM person_table GROUP BY gender\")\n",
    "avg_salary_by_gender.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abbb71",
   "metadata": {},
   "source": [
    "## Creating and managing temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "person_data_df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the temporary view\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ccb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a temporary view exists\n",
    "view_exists = spark.catalog.tableExists(\"people\")\n",
    "view_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940cc837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a temporary view\n",
    "spark.catalog.dropTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b43558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a temporary view exists\n",
    "view_exists = spark.catalog.tableExists(\"people\")\n",
    "view_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ccb229",
   "metadata": {},
   "source": [
    "## SQL Subquries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "employee_data = [\n",
    "    (1, \"John\"), (2, \"Alice\"), (3, \"Bob\"), (4, \"Emily\"),\n",
    "    (5, \"David\"), (6, \"Sarah\"), (7, \"Michael\"), (8, \"Lisa\"),\n",
    "    (9, \"William\")\n",
    "]\n",
    "employees = spark.createDataFrame(employee_data, [\"id\", \"name\"])\n",
    "\n",
    "salary_data = [\n",
    "    (\"HR\", 1, 60000), (\"HR\", 2, 55000), (\"HR\", 3, 58000),\n",
    "    (\"IT\", 4, 70000), (\"IT\", 5, 72000), (\"IT\", 6, 68000),\n",
    "    (\"Sales\", 7, 75000), (\"Sales\", 8, 78000), (\"Sales\", 9, 77000)\n",
    "]\n",
    "salaries = spark.createDataFrame(salary_data, [\"department\", \"id\", \"salary\"])\n",
    "\n",
    "employees.show()\n",
    "\n",
    "salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22776672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "salaries.createOrReplaceTempView(\"salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3aa127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subquery to find employees with salaries above average\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name\n",
    "    FROM employees\n",
    "    WHERE id IN (\n",
    "        SELECT id\n",
    "        FROM salaries\n",
    "        WHERE salary > (SELECT AVG(salary) FROM salaries)\n",
    "    )\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cbdb3a",
   "metadata": {},
   "source": [
    "### Working with Window Functions in PySpark\n",
    "\n",
    "#### Reference\n",
    "https://www.analyticsvidhya.com/blog/2024/03/working-with-window-functions-in-pyspark/#:~:text=Approach%20for%20PySpark%20code,sal%E2%80%9D%20column%20in%20descending%20order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ec854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5875706",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_salary = spark.sql(\"\"\"\n",
    "    select  salaries.*, employees.name\n",
    "    from salaries \n",
    "    left join employees on salaries.id = employees.id\n",
    "\"\"\")\n",
    "\n",
    "employee_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rank of employees within each department based on salary\n",
    "employee_salary.withColumn(\"rank\", F.rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03085dd3-d029-460f-b4f9-7e4e22fe37d4",
   "metadata": {},
   "source": [
    "## Reference Link\n",
    "https://www.analyticsvidhya.com/blog/2024/03/working-with-window-functions-in-pyspark/#:~:text=Approach%20for%20PySpark%20code,sal%E2%80%9D%20column%20in%20descending%20order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6072f-1319-4c2e-b61d-9b2044d32255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample Dataframe\n",
    "employees = [\n",
    "    (7369, \"SMITH\", \"CLERK\", \"17-Dec-80\", 800, 20, 10),\n",
    "    (7499, \"ALLEN\", \"SALESMAN\", \"20-Feb-81\", 1600, 300, 30),\n",
    "    (7521, \"WARD\", \"SALESMAN\", \"22-Feb-81\", 1250, 500, 30),\n",
    "    (7566, \"JONES\", \"MANAGER\", \"2-Apr-81\", 2975, 0, 20),\n",
    "    (7654, \"MARTIN\", \"SALESMAN\", \"28-Sep-81\", 1250, 1400, 30),\n",
    "    (7698, \"BLAKE\", \"MANAGER\", \"1-May-81\", 2850, 0, 30),\n",
    "    (7782, \"CLARK\", \"MANAGER\", \"9-Jun-81\", 2450, 0, 10),\n",
    "    (7788, \"SCOTT\", \"ANALYST\", \"19-Apr-87\", 3000, 0, 20),\n",
    "    (7629, \"ALEX\", \"SALESMAN\", \"28-Sep-79\", 1150, 1400, 30),\n",
    "    (7839, \"KING\", \"PRESIDENT\", \"17-Nov-81\", 5000, 0, 10),\n",
    "    (7844, \"TURNER\", \"SALESMAN\", \"8-Sep-81\", 1500, 0, 30),\n",
    "    (7876, \"ADAMS\", \"CLERK\", \"23-May-87\", 1100, 0, 20)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d51448-f099-4bd6-924d-cd692430125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "emp_df = spark.createDataFrame(employees, \n",
    "           [\"empno\", \"ename\", \"job\", \"hiredate\", \"sal\", \"comm\", \"deptno\"])\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78752b96-95ad-4847-b419-1d0e4ce5b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the schema\n",
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec90d0-d1be-4acb-a720-90905c4f9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9a4ed-a493-44f0-86ec-0011562f9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spark sql\n",
    "rank_df = spark.sql(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, \n",
    "        RANK() OVER (PARTITION BY deptno ORDER BY sal DESC) AS rank FROM emp\"\"\")\n",
    "rank_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63854491-5ad1-4595-8b00-3be31ad677a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PySpark\n",
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())\n",
    "ranking_result_df = emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.rank().over(windowSpec).alias('rank'))\n",
    "ranking_result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487aec6-4d6c-4adc-9617-d99e86b31c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark SQL\n",
    "dense_df = spark.sql(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, \n",
    "        DENSE_RANK() OVER (PARTITION BY deptno ORDER BY sal DESC) \n",
    "        AS dense_rank FROM emp\"\"\")\n",
    "dense_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960611ec-dc46-4f9f-bded-d3a16686cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PySpark\n",
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())\n",
    "dense_ranking_df=emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.dense_rank().over(windowSpec).alias('dense_rank'))\n",
    "dense_ranking_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083c0f8-588d-4c65-a114-8a394f706bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark SQL \n",
    "row_df = spark.sql(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, \n",
    "        ROW_NUMBER() OVER (PARTITION BY deptno ORDER BY sal DESC)\n",
    "         AS row_num FROM emp \"\"\")\n",
    "row_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e3c00-8659-4eb1-83e6-0abc0f3b8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PySpark code\n",
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())\n",
    "row_num_df = emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', \n",
    "               F.row_number().over(windowSpec).alias('row_num'))\n",
    "row_num_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10498ac-57a4-463d-a6d2-3ce0528e069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark SQL\n",
    "running_sum_df = spark.sql(\n",
    "          \"\"\"SELECT empno, ename, job, deptno, sal, \n",
    "          SUM(sal) OVER (PARTITION BY deptno ORDER BY sal DESC) \n",
    "          AS running_total FROM emp\n",
    "          \"\"\")\n",
    "running_sum_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30019158-f570-468a-92e9-2e107ff52453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PySpar\n",
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())\n",
    "running_sum_sal_df= emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', \n",
    "                         F.sum('sal').over(windowSpec).alias('running_total'))\n",
    "running_sum_sal_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2da29-4cf1-4478-94b9-00aa1056b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark SQL\n",
    "next_sal_df = spark.sql(\n",
    "    \"\"\"SELECT empno, ename, job, deptno, sal, LEAD(sal, 1) \n",
    "    OVER (PARTITION BY deptno ORDER BY sal DESC) AS next_val FROM emp\n",
    "    \"\"\")\n",
    "next_sal_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad51470-4b01-4520-8609-6b377f4f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PySpark\n",
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())\n",
    "next_salary_df = emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', \n",
    "               F.lead('sal', offset=1, default=0).over(windowSpec).alias('next_val'))\n",
    "next_salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47d87f-4a26-4420-93d2-6d8335dbc3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fefa0e-ea8e-4e19-a29e-5a10ecfd1e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2031eef-05c2-41f5-8d47-c9ff0ad4a280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddcd44-778b-49c1-b894-a3ed95d5ba4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ddc68f-6d08-45e4-a545-a2e6a5e47bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6113d83",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "### pySpark Tutorials \n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "\n",
    "https://medium.com/codex/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30\n",
    "\n",
    "https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm\n",
    "\n",
    "https://blog.devgenius.io/pyspark-for-begineers-part-3-pyspark-dataframe-db02f0fcd275\n",
    "\n",
    "\n",
    "### Big Data Hadoop Setup & Configuration and Command Line Data processing \n",
    "\n",
    "https://medium.com/@jonty2245/install-apache-hadoop-on-windows-11-a-beginners-guide-45a149f47f8a\n",
    "\n",
    "https://intellipaat.com/blog/tutorial/hadoop-tutorial/hdfs-operations/\n",
    "\n",
    "https://www.projectpro.io/hadoop-tutorial/hadoop-hdfs-commands\n",
    "\n",
    "https://www.geeksforgeeks.org/hdfs-commands/\n",
    "\n",
    "https://medium.com/@ashwin_kumar_/hadoop-hdfs-commands-with-examples-and-usage-570038cbef07\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
