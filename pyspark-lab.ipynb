{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6058785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OS package\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "acb6bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the PySpark environment veriable\n",
    "os.environ['JAVA_HOME']=\"C:\\Program Files\\Java\\jdk-22\"\n",
    "os.environ['SPARK_HOME'] = \"E:\\spark-3.5.1-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"jupyter\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPS'] = \"notebook\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6f4fcc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Pyspark \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "34e41243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BE50MUO:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark-App-Started</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySpark-App-Started>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"PySpark-App-Started\")\\\n",
    ".config(\"spark.executor.memory\",\"4g\")\\\n",
    ".config(\"spark.sql.shuffle.partitions\",\"4\")\\\n",
    ".config(\"spark.jars\",\"E:\\jdbc-driver\\postgresql-42.7.3.jar\")\\\n",
    ".getOrCreate()\n",
    "# .config(\"spark.driver.extraClassPath\", \"E:\\jdbc-driver\\*\")\\\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "92bdb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame \n",
    "columns = [\"id\", \"name\",\"age\",\"gender\"]\n",
    "data = [(1, \"James\",30,\"M\"), (2, \"Ann\",40,\"F\"),\n",
    "    (3, \"Jeff\",41,\"M\"),(4, \"Jennifer\",20,\"F\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "45b12312",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = spark.sparkContext.parallelize(data).toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ccc0ae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+------+\n",
      "| id|    name|age|gender|\n",
      "+---+--------+---+------+\n",
      "|  1|   James| 30|     M|\n",
      "|  2|     Ann| 40|     F|\n",
      "|  3|    Jeff| 41|     M|\n",
      "|  4|Jennifer| 20|     F|\n",
      "+---+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f7125124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 36|\n",
      "|Charlee| 80|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test the setup\n",
    "data1 = [(\"Alice\", 25), (\"Bob\", 36),(\"Charlee\", 80)]\n",
    "df1 = spark.createDataFrame(data1, [\"Name\", \"Age\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58050be",
   "metadata": {},
   "source": [
    "### Database Connection Postgress using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "90092de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = 'jdbc:postgresql://localhost:5432/mldb'\n",
    "props = {'user': 'postgres',\n",
    "              'password': 'postgres',\n",
    "               'driver': 'org.postgresql.Driver'\n",
    "             }\n",
    "sql_df = spark.read.jdbc(url=jdbc_url, table='employee', properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "47455aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "| id|             name|        company|age|             address|  salary|\n",
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "|101|    Krishna Rajan|      Accenture| 35|#Banglaore sasadr...|120000.0|\n",
      "|102|     Surendra Rai|            IBM| 40|#Banglaore, singa...|180000.0|\n",
      "|103|       Goopy Nath|Mphasis Limited| 35|#Banglaore, RT Na...| 90000.0|\n",
      "|104|     Manish Thapa|Mantree Limited| 35|#Banglaore Bamana...|102000.0|\n",
      "|105|Bina Kumar jhakri|            IBM| 35|#Banglaore HSR La...|150000.0|\n",
      "|106|   Roshan Chetrry|       Indexcel| 35|#Banglaore, Korma...|110000.0|\n",
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5255bcf",
   "metadata": {},
   "source": [
    "RDDs( Resilient Distributed Datasets)\n",
    "\n",
    "- Backbone of Data Processing in Spark\n",
    "- Distributed , fault-tolerant, and parallelizable data structure\n",
    "- Efficiently Processes Large datasets across a cluster\n",
    "- key characteristrics : Immutable , Distributed , resilient, lazily evaluated , fault-tolerant operation ( Map, filter, reduce, collect, count , save, map)\n",
    "\n",
    "Transformations and action\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "207c447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple Array list\n",
    "list_number = [2,5,6,7,8,9]\n",
    "\n",
    "#Create the RDD List\n",
    "rdd = sc.parallelize(list_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a49242f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect action  will Retrive all elment of the RDD\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0efde60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create RDD List Tuples\n",
    "list_data = [(\"Alice\", 26), (\"Nishant\", 30),(\"Minika\", 35), (\"Abrain\", 25),(\"Bishnu\", 40), (\"surendra\", 42), (\"tripty\", 24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6b8f0266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 26),\n",
       " ('Nishant', 30),\n",
       " ('Minika', 35),\n",
       " ('Abrain', 25),\n",
       " ('Bishnu', 40),\n",
       " ('surendra', 42),\n",
       " ('tripty', 24)]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2dadfe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b9d2f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the Elment of RDD:  [('Alice', 26), ('Nishant', 30), ('Minika', 35), ('Abrain', 25), ('Bishnu', 40), ('surendra', 42), ('tripty', 24)]\n"
     ]
    }
   ],
   "source": [
    "# collect action to retrive all the elment of RDDS\n",
    "print(\"All the Elment of RDD: \", rdd1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd40072",
   "metadata": {},
   "source": [
    "### RDDS Opetion and Action for Data computation in Distributed system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "64da8332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number RDD Elment in rdd1 Object: 7\n"
     ]
    }
   ],
   "source": [
    "#Count action: it will count the number of Elemnt in the RDD\n",
    "count = rdd1.count()\n",
    "print(\"Total Number RDD Elment in rdd1 Object:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "00fa8ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fist Elemtnt of rdd1 is : ('Alice', 26)\n"
     ]
    }
   ],
   "source": [
    "# First Action : Retrive the first Elment of RDD\n",
    "first_elm = rdd1.first()\n",
    "print(\"Fist Elemtnt of rdd1 is :\", first_elm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3f2bafb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two Elemtnt of rdd1 is : [('Alice', 26), ('Nishant', 30)]\n",
      "four Elemtnt of rdd1 is : [('Alice', 26), ('Nishant', 30), ('Minika', 35), ('Abrain', 25)]\n",
      "fize Elemtnt of rdd1 is : [('Alice', 26), ('Nishant', 30), ('Minika', 35), ('Abrain', 25), ('Bishnu', 40)]\n"
     ]
    }
   ],
   "source": [
    "# take action : its retrive specifyed number of element from RDD Object\n",
    "two_elm = rdd1.take(2)\n",
    "four_elm = rdd1.take(4)\n",
    "five_elm = rdd1.take(5)\n",
    "\n",
    "print(\"two Elemtnt of rdd1 is :\", two_elm)\n",
    "print(\"four Elemtnt of rdd1 is :\", four_elm)\n",
    "print(\"fize Elemtnt of rdd1 is :\", five_elm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "21f762b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach  action will  just print all elment of RDD\n",
    "def show_rdd(item):\n",
    "    print(item)\n",
    "    \n",
    "rdd1.foreach(show_rdd)\n",
    "#or \n",
    "rdd1.foreach(lambda x : print(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4f1ab",
   "metadata": {},
   "source": [
    "### Rdd Operation : Transformation  using spark method like ( map, filter, reducebykey, sortBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f8761c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the Name Upper Case:  [('ALICE', 26), ('NISHANT', 30), ('MINIKA', 35), ('ABRAIN', 25), ('BISHNU', 40), ('SURENDRA', 42), ('TRIPTY', 24)]\n",
      "All the Name Title Case:  [('Alice', 26), ('Nishant', 30), ('Minika', 35), ('Abrain', 25), ('Bishnu', 40), ('Surendra', 42), ('Tripty', 24)]\n"
     ]
    }
   ],
   "source": [
    "#Map transformation : conver the name to uppercase \n",
    "map_rdd = rdd1.map(lambda x : ( x[0].upper(), x[1])) \n",
    "result = map_rdd.collect()\n",
    "print(\"All the Name Upper Case: \", result)\n",
    "\n",
    "map_rdd1 = rdd1.map(lambda x : ( x[0].title(), x[1])) \n",
    "result_title = map_rdd1.collect()\n",
    "print(\"All the Name Title Case: \", result_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "12c22804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Minika', 35), ('Bishnu', 40), ('surendra', 42)]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter transformation Filter the where as the greaten then 30\n",
    "filter_rdd = rdd1.filter(lambda x : x[1] > 30)\n",
    "filter_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "103feafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tripty', 24),\n",
       " ('Bishnu', 40),\n",
       " ('surendra', 42),\n",
       " ('Minika', 35),\n",
       " ('Abrain', 25),\n",
       " ('Nishant', 30),\n",
       " ('Alice', 26)]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reduce by key Transformation :Calculate the total age of each element\n",
    "redurce_rdd= rdd1.reduceByKey(lambda x, y: x + y)\n",
    "redurce_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "762fbfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('surendra', 42),\n",
       " ('Bishnu', 40),\n",
       " ('Minika', 35),\n",
       " ('Nishant', 30),\n",
       " ('Alice', 26),\n",
       " ('Abrain', 25),\n",
       " ('tripty', 24)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortby Transformation : it will sort rdd by age decending order\n",
    "sortby_rdd = rdd1.sortBy(lambda x : x[1], ascending=False)\n",
    "sortby_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7cb020a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'java',\n",
       " 'javaScript',\n",
       " 'HTML',\n",
       " 'Python',\n",
       " 'go',\n",
       " 'languge',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark',\n",
       " 'vs',\n",
       " 'hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark',\n",
       " 'and',\n",
       " 'spark']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatMap() method Transfomation here string of array word spliging by space\n",
    "words_rdd = sc.parallelize (\n",
    "  [\"scala\", \n",
    "   \"java\", \n",
    "   \"javaScript\",\n",
    "   \"HTML\",\n",
    "   \"Python\",\n",
    "   \"go languge\",\n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_rdd.flatMap(lambda x: x.split(\" \")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b9e049a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Partitions Before:  8\n"
     ]
    }
   ],
   "source": [
    "# .repartition(n) — makes n number of partitions on RDD\n",
    "print(\"Number of Partitions Before: \",words_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b2950136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'java',\n",
       " 'javaScript',\n",
       " 'HTML',\n",
       " 'Python',\n",
       " 'go languge',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark vs hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "4eebb485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chandu', 1, 'rohith', 2, 'minu', 3, 'karthik', 4]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd = spark.sparkContext.parallelize([\"chandu\",1,\"rohith\",2,\"minu\",3,\"karthik\",4])\n",
    "my_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1aaa3652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chandu', 1, 'rohith', 2]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9fcf4b",
   "metadata": {},
   "source": [
    "### Read and Write Rdd From the Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9836683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SaveAsTextFile Rdd \n",
    "#rdd1.saveAsTextFile('data/rdd1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9c6639ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('Alice', 26)\",\n",
       " \"('Nishant', 30)\",\n",
       " \"('Minika', 35)\",\n",
       " \"('Abrain', 25)\",\n",
       " \"('Bishnu', 40)\",\n",
       " \"('surendra', 42)\",\n",
       " \"('tripty', 24)\"]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading TextFile RDD\n",
    "text_file_rdd  = sc.textFile('data/rdd1.txt')\n",
    "text_file_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3ef668f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getNumPartitions() — returns the number of Partitions\n",
    "text_file_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "68425426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dataframe Csv Data\n",
    "df_pyspark = spark.read.csv(\"data/Amazon_Sale_Report.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e5cd57fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+--------+--------------------+----------+--------------+------------------+--------+-------------------+-------------+----+----------+--------------+---+--------+------+-----------+--------------+----------------+------------+--------------------+-----+------------+-----------+\n",
      "|index|           Order ID|    Date|              Status|Fulfilment|Sales Channel |ship-service-level|   Style|                SKU|     Category|Size|      ASIN|Courier Status|Qty|currency|Amount|  ship-city|    ship-state|ship-postal-code|ship-country|       promotion-ids|  B2B|fulfilled-by|Unnamed: 22|\n",
      "+-----+-------------------+--------+--------------------+----------+--------------+------------------+--------+-------------------+-------------+----+----------+--------------+---+--------+------+-----------+--------------+----------------+------------+--------------------+-----+------------+-----------+\n",
      "|    0|405-8078784-5731545|04-30-22|           Cancelled|  Merchant|     Amazon.in|          Standard|  SET389|     SET389-KR-NP-S|          Set|   S|B09KXVBD7Z|          NULL|  0|     INR|647.62|     MUMBAI|   MAHARASHTRA|        400081.0|          IN|                NULL|false|   Easy Ship|       NULL|\n",
      "|    1|171-9198151-1101146|04-30-22|Shipped - Deliver...|  Merchant|     Amazon.in|          Standard| JNE3781|    JNE3781-KR-XXXL|        kurta| 3XL|B09K3WFS32|       Shipped|  1|     INR| 406.0|  BENGALURU|     KARNATAKA|        560085.0|          IN|Amazon PLCC Free-...|false|   Easy Ship|       NULL|\n",
      "|    2|404-0687676-7273146|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited| JNE3371|      JNE3371-KR-XL|        kurta|  XL|B07WV4JV4D|       Shipped|  1|     INR| 329.0|NAVI MUMBAI|   MAHARASHTRA|        410210.0|          IN|IN Core Free Ship...| true|        NULL|       NULL|\n",
      "|    3|403-9615377-8133951|04-30-22|           Cancelled|  Merchant|     Amazon.in|          Standard|   J0341|         J0341-DR-L|Western Dress|   L|B099NRCT7B|          NULL|  0|     INR|753.33| PUDUCHERRY|    PUDUCHERRY|        605008.0|          IN|                NULL|false|   Easy Ship|       NULL|\n",
      "|    4|407-1069790-7240320|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited| JNE3671|    JNE3671-TU-XXXL|          Top| 3XL|B098714BZP|       Shipped|  1|     INR| 574.0|    CHENNAI|    TAMIL NADU|        600073.0|          IN|                NULL|false|        NULL|       NULL|\n",
      "|    5|404-1490984-4578765|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited|  SET264|    SET264-KR-NP-XL|          Set|  XL|B08YN7XDSG|       Shipped|  1|     INR| 824.0|  GHAZIABAD| UTTAR PRADESH|        201102.0|          IN|IN Core Free Ship...|false|        NULL|       NULL|\n",
      "|    6|408-5748499-6859555|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited|   J0095|        J0095-SET-L|          Set|   L|B08CMHNWBN|       Shipped|  1|     INR| 653.0| CHANDIGARH|    CHANDIGARH|        160036.0|          IN|IN Core Free Ship...|false|        NULL|       NULL|\n",
      "|    7|406-7807733-3785945|04-30-22|Shipped - Deliver...|  Merchant|     Amazon.in|          Standard| JNE3405|       JNE3405-KR-S|        kurta|   S|B081WX4G4Q|       Shipped|  1|     INR| 399.0|  HYDERABAD|     TELANGANA|        500032.0|          IN|Amazon PLCC Free-...|false|   Easy Ship|       NULL|\n",
      "|    8|407-5443024-5233168|04-30-22|           Cancelled|    Amazon|     Amazon.in|         Expedited|  SET200|SET200-KR-NP-A-XXXL|          Set| 3XL|B08L91ZZXN|     Cancelled|  0|    NULL|  NULL|  HYDERABAD|     TELANGANA|        500008.0|          IN|IN Core Free Ship...|false|        NULL|       NULL|\n",
      "|    9|402-4393761-0311520|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited| JNE3461|     JNE3461-KR-XXL|        kurta| XXL|B08B3XF5MH|       Shipped|  1|     INR| 363.0|    Chennai|    TAMIL NADU|        600041.0|          IN|                NULL|false|        NULL|       NULL|\n",
      "|   10|407-5633625-6970741|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited| JNE3160|     JNE3160-KR-G-S|        kurta|   S|B07K3YQLF1|       Shipped|  1|     INR| 685.0|    CHENNAI|    TAMIL NADU|        600073.0|          IN|                NULL|false|        NULL|       NULL|\n",
      "|   11|171-4638481-6326716|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited| JNE3500|      JNE3500-KR-XS|        kurta|  XS|B098117DJ3|       Shipped|  1|     INR| 364.0|      NOIDA| UTTAR PRADESH|        201303.0|          IN|                NULL|false|        NULL|       NULL|\n",
      "|   12|405-5513694-8146768|04-30-22|Shipped - Deliver...|  Merchant|     Amazon.in|          Standard| JNE3405|      JNE3405-KR-XS|        kurta|  XS|B081XCMYXJ|       Shipped|  1|     INR| 399.0|  Amravati.|   MAHARASHTRA|        444606.0|          IN|Amazon PLCC Free-...|false|   Easy Ship|       NULL|\n",
      "|   13|408-7955685-3083534|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited|  SET182|    SET182-KR-DH-XS|          Set|  XS|B085HS947T|       Shipped|  1|     INR| 657.0|     MUMBAI|   MAHARASHTRA|        400053.0|          IN|                NULL|false|        NULL|       NULL|\n",
      "|   14|408-1298370-1920302|04-30-22|Shipped - Deliver...|  Merchant|     Amazon.in|          Standard|   J0351|        J0351-SET-L|          Set|   L|B09CSSQY4F|       Shipped|  1|     INR| 771.0|     MUMBAI|   MAHARASHTRA|        400053.0|          IN|Amazon PLCC Free-...|false|   Easy Ship|       NULL|\n",
      "|   15|403-4965581-9520319|04-30-22|Shipped - Deliver...|  Merchant|     Amazon.in|          Standard|PJNE3368|    PJNE3368-KR-6XL|        kurta| 6XL|B09PY99SVJ|       Shipped|  1|     INR| 544.0|   GUNTAKAL|ANDHRA PRADESH|        515801.0|          IN|Amazon PLCC Free-...|false|   Easy Ship|       NULL|\n",
      "|   16|406-9379318-6555504|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited| JNE3721|     JNE3721-KR-XXL|        kurta| XXL|B099FCT65D|       Shipped|  1|     INR| 329.0|     JAIPUR|     RAJASTHAN|        302020.0|          IN|IN Core Free Ship...|false|        NULL|       NULL|\n",
      "|   17|405-9013803-8009918|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited| JNE3405|      JNE3405-KR-XL|        kurta|  XL|B081WT6GG7|       Shipped|  1|     INR| 399.0|  NEW DELHI|         DELHI|        110074.0|          IN|                NULL|false|        NULL|       NULL|\n",
      "|   18|402-4030358-5835511|04-30-22|Shipped - Deliver...|  Merchant|     Amazon.in|          Standard| JNE3697|     JNE3697-KR-XXL|        kurta| XXL|B098133PV5|       Shipped|  1|     INR| 458.0|    Gurgaon|       HARYANA|        122004.0|          IN|Amazon PLCC Free-...|false|   Easy Ship|       NULL|\n",
      "|   19|405-5957858-1051546|04-30-22|             Shipped|    Amazon|     Amazon.in|         Expedited|  SET254|    SET254-KR-NP-XS|          Set|  XS|B0983DDPL6|       Shipped|  1|     INR| 886.0|  BENGALURU|     KARNATAKA|        560017.0|          IN|                NULL|false|        NULL|       NULL|\n",
      "+-----+-------------------+--------+--------------------+----------+--------------+------------------+--------+-------------------+-------------+----+----------+--------------+---+--------+------+-----------+--------------+----------------+------------+--------------------+-----+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "08687b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Fulfilment: string (nullable = true)\n",
      " |-- Sales Channel : string (nullable = true)\n",
      " |-- ship-service-level: string (nullable = true)\n",
      " |-- Style: string (nullable = true)\n",
      " |-- SKU: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- ASIN: string (nullable = true)\n",
      " |-- Courier Status: string (nullable = true)\n",
      " |-- Qty: integer (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- ship-city: string (nullable = true)\n",
      " |-- ship-state: string (nullable = true)\n",
      " |-- ship-postal-code: double (nullable = true)\n",
      " |-- ship-country: string (nullable = true)\n",
      " |-- promotion-ids: string (nullable = true)\n",
      " |-- B2B: boolean (nullable = true)\n",
      " |-- fulfilled-by: string (nullable = true)\n",
      " |-- Unnamed: 22: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To view the all the schemal\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "70218ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128975"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataFrame  the number of rows in a DataFrame\n",
    "df_pyspark.count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0e4b2660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['index', 'Order ID', 'Date', 'Status', 'Fulfilment', 'Sales Channel ', 'ship-service-level', 'Style', 'SKU', 'Category', 'Size', 'ASIN', 'Courier Status', 'Qty', 'currency', 'Amount', 'ship-city', 'ship-state', 'ship-postal-code', 'ship-country', 'promotion-ids', 'B2B', 'fulfilled-by', 'Unnamed: 22']\n",
      "Column No: 24\n"
     ]
    }
   ],
   "source": [
    "print(df_pyspark.columns)   # lists all columns in DataFrame\n",
    "print(\"Column No:\",len(df_pyspark.columns))   # lists all columns in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "93cd2dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('index', 'int'), ('Order ID', 'string'), ('Date', 'string'), ('Status', 'string'), ('Fulfilment', 'string'), ('Sales Channel ', 'string'), ('ship-service-level', 'string'), ('Style', 'string'), ('SKU', 'string'), ('Category', 'string'), ('Size', 'string'), ('ASIN', 'string'), ('Courier Status', 'string'), ('Qty', 'int'), ('currency', 'string'), ('Amount', 'double'), ('ship-city', 'string'), ('ship-state', 'string'), ('ship-postal-code', 'double'), ('ship-country', 'string'), ('promotion-ids', 'string'), ('B2B', 'boolean'), ('fulfilled-by', 'string'), ('Unnamed: 22', 'boolean')]\n"
     ]
    }
   ],
   "source": [
    "#checking Data types\n",
    "print(df_pyspark.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "ae3b3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#describing Numerical method\n",
    "#df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "301bf32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+---+------+\n",
      "|     Category|Size|Qty|Amount|\n",
      "+-------------+----+---+------+\n",
      "|          Set|   S|  0|647.62|\n",
      "|        kurta| 3XL|  1| 406.0|\n",
      "|        kurta|  XL|  1| 329.0|\n",
      "|Western Dress|   L|  0|753.33|\n",
      "|          Top| 3XL|  1| 574.0|\n",
      "|          Set|  XL|  1| 824.0|\n",
      "|          Set|   L|  1| 653.0|\n",
      "|        kurta|   S|  1| 399.0|\n",
      "|          Set| 3XL|  0|  NULL|\n",
      "|        kurta| XXL|  1| 363.0|\n",
      "|        kurta|   S|  1| 685.0|\n",
      "|        kurta|  XS|  1| 364.0|\n",
      "|        kurta|  XS|  1| 399.0|\n",
      "|          Set|  XS|  1| 657.0|\n",
      "|          Set|   L|  1| 771.0|\n",
      "|        kurta| 6XL|  1| 544.0|\n",
      "|        kurta| XXL|  1| 329.0|\n",
      "|        kurta|  XL|  1| 399.0|\n",
      "|        kurta| XXL|  1| 458.0|\n",
      "|          Set|  XS|  1| 886.0|\n",
      "+-------------+----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select([\"Category\",\"Size\",\"Qty\", \"Amount\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "09fdd34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|     Category|\n",
      "+-------------+\n",
      "|        kurta|\n",
      "| Ethnic Dress|\n",
      "|        Saree|\n",
      "|       Blouse|\n",
      "|          Top|\n",
      "|       Bottom|\n",
      "|          Set|\n",
      "|Western Dress|\n",
      "|      Dupatta|\n",
      "+-------------+\n",
      "\n",
      "Total District product No: 9\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select([\"Category\"]).distinct().show()\n",
    "print(\"Total District product No:\",df_pyspark.select([\"Category\"]).distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42376774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f8a9ae3",
   "metadata": {},
   "source": [
    "## RDD to DataFrame and Viceversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fecef0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|      _1| _2|\n",
      "+--------+---+\n",
      "|   Alice| 26|\n",
      "| Nishant| 30|\n",
      "|  Minika| 35|\n",
      "|  Abrain| 25|\n",
      "|  Bishnu| 40|\n",
      "|surendra| 42|\n",
      "|  tripty| 24|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert to rdd to dataframe user toDF() method\n",
    "rdd_df = rdd1.toDF()\n",
    "rdd_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "831aed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|   Alice| 26|\n",
      "| Nishant| 30|\n",
      "|  Minika| 35|\n",
      "|  Abrain| 25|\n",
      "|  Bishnu| 40|\n",
      "|surendra| 42|\n",
      "|  tripty| 24|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using createDataFrame() - Convert DataFrame to RDD\n",
    "rdd_df2 = spark.createDataFrame(rdd1).toDF(\"Name\",\"Age\")\n",
    "rdd_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d49afdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to RDD\n",
    "rdd_from_df = rdd_df2.rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7518e70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Alice', Age=26),\n",
       " Row(Name='Nishant', Age=30),\n",
       " Row(Name='Minika', Age=35),\n",
       " Row(Name='Abrain', Age=25),\n",
       " Row(Name='Bishnu', Age=40),\n",
       " Row(Name='surendra', Age=42),\n",
       " Row(Name='tripty', Age=24)]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_from_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909f28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bbda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8711a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5df4f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data base Interface\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9bd88",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://sparkbyexamples.com/pyspark-rdd/\n",
    "\n",
    "https://www.youtube.com/watch?v=EB8lfdxpirM\n",
    "\n",
    "https://pub.towardsai.net/pyspark-for-beginners-part-1-introduction-638fb16c5092\n",
    "\n",
    "https://medium.com/codex/pyspark-for-beginners-part-4-pyspark-rdd-7b5587347b4c\n",
    "\n",
    "https://medium.com/codex/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30\n",
    "\n",
    "https://blog.devgenius.io/pyspark-for-begineers-part-3-pyspark-dataframe-db02f0fcd275"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
