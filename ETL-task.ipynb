{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ec068fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b39393d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the PySpark environment veriable\n",
    "os.environ['JAVA_HOME']=\"E:\\jdk-22\"\n",
    "os.environ['SPARK_HOME'] = \"E:\\spark-3.5.1-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"jupyter\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPS'] = \"notebook\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dcd36392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sart the Sparksession\n",
    "spark = SparkSession.builder.appName(\"Practice\")\\\n",
    ".config(\"spark.jars\",\"E:\\jdbc-driver\\postgresql-42.7.3.jar\")\\\n",
    ".config(\"spark.jars.packages\",\"com.mysql:mysql-connector-j:8.4.0\")\\\n",
    ".enableHiveSupport()\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4516e22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BE50MUO:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Practice>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a58b7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query table using jdbc()\n",
    "df = spark.read\\\n",
    "    .jdbc(\"jdbc:mysql://localhost:3306/park\", \"item\",\\\n",
    "          properties={\"user\": \"root\", \"password\": \"root@123\", \"driver\":\"com.mysql.cj.jdbc.Driver\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a4d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "72642bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+----+---+-----+\n",
      "| id|product_name|category|size|qty|price|\n",
      "+---+------------+--------+----+---+-----+\n",
      "|  1|        pent| trauser|  32| 10|  250|\n",
      "|  2|        pent| trauser|  38| 10|  250|\n",
      "|  3|        pent| trauser|  30| 10|  250|\n",
      "|  4|        pent| trauser|  32| 10|  250|\n",
      "|  5|     T-short| T-short|  32| 10|  250|\n",
      "|  6|  half-short| T-short|  30| 10|  250|\n",
      "|  7|      snkier|    show|  32| 10|  250|\n",
      "|  8|      snkier| trauser|  30| 10|  250|\n",
      "|  9|      snkier| trauser|  30| 10|  250|\n",
      "| 10|      snkier| trauser|  35| 10|  250|\n",
      "| 11|        pent| trauser|  32| 10|  250|\n",
      "| 12|        pent| trauser|  32| 10|  250|\n",
      "| 13|        pent| trauser|  32| 10|  250|\n",
      "+---+------------+--------+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f0b7cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = 'jdbc:postgresql://localhost:5432/mldb'\n",
    "props = {'user': 'postgres',\n",
    "              'password': 'postgres',\n",
    "               'driver': 'org.postgresql.Driver'\n",
    "             }\n",
    "sql_df = spark.read.jdbc(url=jdbc_url, table='employee', properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "845e18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "| id|             name|        company|age|             address|  salary|\n",
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "|101|    Krishna Rajan|      Accenture| 35|#Banglaore sasadr...|120000.0|\n",
      "|102|     Surendra Rai|            IBM| 40|#Banglaore, singa...|180000.0|\n",
      "|103|       Goopy Nath|Mphasis Limited| 35|#Banglaore, RT Na...| 90000.0|\n",
      "|104|     Manish Thapa|Mantree Limited| 35|#Banglaore Bamana...|102000.0|\n",
      "|105|Bina Kumar jhakri|            IBM| 35|#Banglaore HSR La...|150000.0|\n",
      "|106|   Roshan Chetrry|       Indexcel| 35|#Banglaore, Korma...|110000.0|\n",
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eaa6c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "# PostgreSQL connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/mldb\"\n",
    "connection_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "# Fetch all table names from the PostgreSQL database\n",
    "def get_table_names():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"mldb\",\n",
    "        user=\"postgres\",\n",
    "        password=\"postgres\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"SELECT table_name FROM information_schema.tables WHERE table_schema='public';\"\n",
    "    )\n",
    "    tables = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return [table[0] for table in tables]\n",
    "\n",
    "# Read all tables into PySpark DataFrames\n",
    "table_names = get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a12f5000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         |  df_item|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b248d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_table = pd.DataFrame({'Tables': table_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "885de6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|      Tables|\n",
      "+------------+\n",
      "|    employee|\n",
      "|       users|\n",
      "|user_message|\n",
      "|        post|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_all_table).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "47e29973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                name|\n",
      "+---+--------------------+\n",
      "|  1|surendra         ...|\n",
      "|  2|Bikash thapa     ...|\n",
      "|  3|Ritu Rai         ...|\n",
      "|  4|Minakshi         ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df = spark.read.jdbc(url=jdbc_url, table='users', properties=connection_properties)\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a780e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+--------------------+--------------------+--------------------+\n",
      "| id|                uuid|       username|             message|          created_at|          updated_at|\n",
      "+---+--------------------+---------------+--------------------+--------------------+--------------------+\n",
      "|  1|1719f9a6-a6f8-460...|   prity thapat|He is a full stac...|2023-02-11 21:01:...|2023-02-11 21:01:...|\n",
      "|  2|590e1b1c-cae8-48a...|  krisha thapat|He is a full stac...|2023-02-11 21:02:...|2023-02-11 21:02:...|\n",
      "|  3|3ffcd235-81e7-4bc...|   Binah thapat|He is a full stac...|2023-02-11 21:02:...|2023-02-11 21:02:...|\n",
      "|  4|4067c132-3e25-492...|     sita thapa|He is a full stac...|2023-02-11 21:02:...|2023-02-11 21:02:...|\n",
      "|  6|03cad32e-1e40-4be...|  binisha thapa|He is a full stac...|2023-02-11 21:02:...|2023-02-11 21:02:...|\n",
      "|  8|8077dafc-1e8b-477...|   Bikash thapa|He is a full stac...|2023-02-11 21:03:...|2023-02-11 21:03:...|\n",
      "| 10|9ccbe731-18cb-495...| diskarta thapa|He is a full stac...|2023-02-11 21:21:...|2023-02-11 21:21:...|\n",
      "| 11|7a3efe2b-363d-422...|Miniakshi thapa|He is a full stac...|2023-02-11 21:21:...|2023-02-11 21:21:...|\n",
      "| 12|c3a03e29-21d2-40c...|   libita thapa|He is a full stac...|2023-02-11 21:21:...|2023-02-11 21:21:...|\n",
      "| 14|e3c1cc2a-204b-4d6...|     Bikash rai|Sujina and bikash...|2023-02-11 22:58:...|2023-02-11 22:58:...|\n",
      "| 13|49afbce1-fcc7-46d...|     sunita rai|she is from nepal...|2023-02-11 22:52:...|2023-02-12 03:04:...|\n",
      "+---+--------------------+---------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message_df = spark.read.jdbc(url=jdbc_url, table='post', properties=connection_properties)\n",
    "user_message_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "96f2e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "| id|             name|        company|age|             address|  salary|\n",
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "|101|    Krishna Rajan|      Accenture| 35|#Banglaore sasadr...|120000.0|\n",
      "|102|     Surendra Rai|            IBM| 40|#Banglaore, singa...|180000.0|\n",
      "|103|       Goopy Nath|Mphasis Limited| 35|#Banglaore, RT Na...| 90000.0|\n",
      "|104|     Manish Thapa|Mantree Limited| 35|#Banglaore Bamana...|102000.0|\n",
      "|105|Bina Kumar jhakri|            IBM| 35|#Banglaore HSR La...|150000.0|\n",
      "|106|   Roshan Chetrry|       Indexcel| 35|#Banglaore, Korma...|110000.0|\n",
      "+---+-----------------+---------------+---+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df = spark.read.jdbc(url=jdbc_url, table='employee', properties=connection_properties)\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "023f30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# employee_df = spark.read.jdbc(url=jdbc_url, table='user_message', properties=connection_properties)\n",
    "# employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f6cde527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark SQL Data Analysis the project\n",
    "df.createOrReplaceTempView(\"df_item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "57c9edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+----+---+-----+\n",
      "| id|product_name|category|size|qty|price|\n",
      "+---+------------+--------+----+---+-----+\n",
      "|  1|        pent| trauser|  32| 10|  250|\n",
      "|  2|        pent| trauser|  38| 10|  250|\n",
      "|  4|        pent| trauser|  32| 10|  250|\n",
      "|  5|     T-short| T-short|  32| 10|  250|\n",
      "|  7|      snkier|    show|  32| 10|  250|\n",
      "| 10|      snkier| trauser|  35| 10|  250|\n",
      "| 11|        pent| trauser|  32| 10|  250|\n",
      "| 12|        pent| trauser|  32| 10|  250|\n",
      "| 13|        pent| trauser|  32| 10|  250|\n",
      "+---+------------+--------+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM df_item where size > 30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6708dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252aab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
